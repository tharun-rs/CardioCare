{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lSYd2mY-vvI"
      },
      "source": [
        "# Notebook setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchattacks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqNLRCuzkUYt",
        "outputId": "54a6c3c2-0a41-46c1-d2bb-704de1e34401"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchattacks in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (0.17.1+cu121)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (4.66.2)\n",
            "Requirement already satisfied: requests~=2.25.1 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.10/dist-packages (from torchattacks) (1.25.2)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->torchattacks) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->torchattacks) (12.4.127)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->torchattacks) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVWGZOhi-AdA",
        "outputId": "b7f8c7b6-98ad-4572-8e1d-a01db69fe448"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted files: ['train', 'val', 'test']\n",
            "Files Loaded!!!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive') #mount Google Drive\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def copy_folder_contents(source_folder, destination_folder):\n",
        "    for item in os.listdir(source_folder):\n",
        "        item_path = os.path.join(source_folder, item)\n",
        "        destination_path = os.path.join(destination_folder, item)\n",
        "\n",
        "        if os.path.isfile(item_path):\n",
        "            shutil.copy(item_path, destination_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            os.makedirs(destination_path, exist_ok=True)\n",
        "            copy_folder_contents(item_path, destination_path)\n",
        "\n",
        "\n",
        "\n",
        "source_folder = \"/content/drive/My Drive/CardioCare\"\n",
        "destination_folder = \"/content/CardioCare\"\n",
        "\n",
        "\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "#copy folders from drive to runtime\n",
        "copy_folder_contents(source_folder, destination_folder)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get dataset from drive\n",
        "zip_file_path = \"/content/drive/My Drive/ecg.zip\"\n",
        "extraction_path = \"/content/dataset\"\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "extracted_files = os.listdir(extraction_path)\n",
        "print(\"Extracted files:\", extracted_files)\n",
        "\n",
        "\n",
        "print(\"Files Loaded!!!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q2Hj5StYbBe"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for inceptionv3"
      ],
      "metadata": {
        "id": "UrgjowqSUxR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "keYHqhTBUd_h"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = \"/content/dataset/train\"\n",
        "test_dir = \"/content/dataset/test\"\n",
        "val_dir = \"/content/dataset/val\"\n",
        "# Define image transformations (optional)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "# Create the DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for alexnet"
      ],
      "metadata": {
        "id": "b3_yYQYuUtfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hpqTZjviYdkl"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = \"/content/dataset/train\"\n",
        "test_dir = \"/content/dataset/test\"\n",
        "val_dir = \"/content/dataset/val\"\n",
        "# Define image transformations (optional)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "\n",
        "# Create the DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Training (DL Models)"
      ],
      "metadata": {
        "id": "EKc2jG4Qf9Vi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "121I7sZLSix9"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfIE5g9YSmH0",
        "outputId": "a100dbe3-8295-46a7-c753-3e5760a4953d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Devide used: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 152MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Epoch: 1/25, Train Loss: 25.6579, Val Loss: 8.3551,\n",
            "\t\t Val Accuracy: 25.7716, Time Taken: 55.36 seconds\n",
            "Epoch: 2\n",
            "Epoch: 2/25, Train Loss: 4.5524, Val Loss: 8.0789,\n",
            "\t\t Val Accuracy: 45.9877, Time Taken: 54.29 seconds\n",
            "Epoch: 3\n",
            "Epoch: 3/25, Train Loss: 4.2569, Val Loss: 7.6285,\n",
            "\t\t Val Accuracy: 42.7469, Time Taken: 56.11 seconds\n",
            "Epoch: 4\n",
            "Epoch: 4/25, Train Loss: 4.0661, Val Loss: 7.0126,\n",
            "\t\t Val Accuracy: 50.4630, Time Taken: 54.54 seconds\n",
            "Epoch: 5\n",
            "Epoch: 5/25, Train Loss: 3.7751, Val Loss: 6.5410,\n",
            "\t\t Val Accuracy: 55.7099, Time Taken: 54.60 seconds\n",
            "Epoch: 6\n",
            "Epoch: 6/25, Train Loss: 3.4696, Val Loss: 6.2765,\n",
            "\t\t Val Accuracy: 57.4074, Time Taken: 55.26 seconds\n",
            "Epoch: 7\n",
            "Epoch: 7/25, Train Loss: 3.3601, Val Loss: 5.8947,\n",
            "\t\t Val Accuracy: 60.6481, Time Taken: 54.41 seconds\n",
            "Epoch: 8\n",
            "Epoch: 8/25, Train Loss: 3.2105, Val Loss: 5.5500,\n",
            "\t\t Val Accuracy: 65.5864, Time Taken: 54.19 seconds\n",
            "Epoch: 9\n",
            "Epoch: 9/25, Train Loss: 3.0436, Val Loss: 5.2805,\n",
            "\t\t Val Accuracy: 68.3642, Time Taken: 54.73 seconds\n",
            "Epoch: 10\n",
            "Epoch: 10/25, Train Loss: 2.7543, Val Loss: 4.8551,\n",
            "\t\t Val Accuracy: 71.9136, Time Taken: 56.24 seconds\n",
            "Epoch: 11\n",
            "Epoch: 11/25, Train Loss: 2.6489, Val Loss: 4.4803,\n",
            "\t\t Val Accuracy: 73.7654, Time Taken: 55.01 seconds\n",
            "Epoch: 12\n",
            "Epoch: 12/25, Train Loss: 2.4874, Val Loss: 4.1368,\n",
            "\t\t Val Accuracy: 80.0926, Time Taken: 54.37 seconds\n",
            "Epoch: 13\n",
            "Epoch: 13/25, Train Loss: 2.1748, Val Loss: 3.7471,\n",
            "\t\t Val Accuracy: 77.6235, Time Taken: 54.68 seconds\n",
            "Epoch: 14\n",
            "Epoch: 14/25, Train Loss: 1.9598, Val Loss: 3.1629,\n",
            "\t\t Val Accuracy: 84.4136, Time Taken: 54.76 seconds\n",
            "Epoch: 15\n",
            "Epoch: 15/25, Train Loss: 1.8276, Val Loss: 2.7747,\n",
            "\t\t Val Accuracy: 86.7284, Time Taken: 55.05 seconds\n",
            "Epoch: 16\n",
            "Epoch: 16/25, Train Loss: 1.6248, Val Loss: 2.3850,\n",
            "\t\t Val Accuracy: 90.1235, Time Taken: 54.06 seconds\n",
            "Epoch: 17\n",
            "Epoch: 17/25, Train Loss: 1.4141, Val Loss: 2.1448,\n",
            "\t\t Val Accuracy: 88.2716, Time Taken: 55.02 seconds\n",
            "Epoch: 18\n",
            "Epoch: 18/25, Train Loss: 1.2084, Val Loss: 1.7076,\n",
            "\t\t Val Accuracy: 92.2840, Time Taken: 54.83 seconds\n",
            "Epoch: 19\n",
            "Epoch: 19/25, Train Loss: 1.1600, Val Loss: 1.5196,\n",
            "\t\t Val Accuracy: 91.3580, Time Taken: 54.15 seconds\n",
            "Epoch: 20\n",
            "Epoch: 20/25, Train Loss: 0.9396, Val Loss: 1.2083,\n",
            "\t\t Val Accuracy: 94.4444, Time Taken: 54.12 seconds\n",
            "Epoch: 21\n",
            "Epoch: 21/25, Train Loss: 0.8958, Val Loss: 0.9569,\n",
            "\t\t Val Accuracy: 95.9877, Time Taken: 55.62 seconds\n",
            "Epoch: 22\n",
            "Epoch: 22/25, Train Loss: 0.7364, Val Loss: 0.8303,\n",
            "\t\t Val Accuracy: 95.6790, Time Taken: 54.19 seconds\n",
            "Epoch: 23\n",
            "Epoch: 23/25, Train Loss: 0.6353, Val Loss: 0.6832,\n",
            "\t\t Val Accuracy: 96.1420, Time Taken: 54.27 seconds\n",
            "Epoch: 24\n",
            "Epoch: 24/25, Train Loss: 0.6108, Val Loss: 0.5739,\n",
            "\t\t Val Accuracy: 96.4506, Time Taken: 54.69 seconds\n",
            "Epoch: 25\n",
            "Epoch: 25/25, Train Loss: 0.4897, Val Loss: 0.4919,\n",
            "\t\t Val Accuracy: 96.7593, Time Taken: 55.78 seconds\n",
            "Model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "\n",
        "# Load pre-trained AlexNet model\n",
        "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "\n",
        "# Freeze feature extraction layers (optional)\n",
        "for param in alexnet.features[:].parameters():\n",
        "    param.requires_grad = False  # Freeze weights of feature extraction layers\n",
        "# Get the number of features output by the pre-trained alexnet\n",
        "num_ftrs = alexnet.classifier[6].in_features\n",
        "num_classes = 4\n",
        "alexnet.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Define optimizer (e.g., Adam optimizer)\n",
        "optimizer = torch.optim.Adam(alexnet.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "\n",
        "num_epochs = 30\n",
        "alexnet.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    # Train and validate the alexnet\n",
        "    alexnet.train()\n",
        "    start = time.time()\n",
        "    train_loss = 0.0\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    batch = 0;\n",
        "    for image, label in train_loader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      batch += 1\n",
        "      print(f'Batch {batch}/{num_batches}',end='')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = alexnet(image)\n",
        "\n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      print(f'\\r',end='')\n",
        "\n",
        "    # Validate the model\n",
        "    alexnet.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for image, label in val_loader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            output = alexnet(image)\n",
        "            loss = criterion(output, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},\\n\\t\\t Val Accuracy: {val_accuracy:.4f}, Time Taken: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained alexnet\n",
        "torch.save(alexnet.state_dict(), \"/content/CardioCare/alexnet.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWGAs6ru2piu",
        "outputId": "96d73a13-01b9-4dfa-e209-d78a81796a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.0387, Test Accuracy: 98.61%, Testing Time: 26.92 seconds\n"
          ]
        }
      ],
      "source": [
        "# Testing the model\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100 * correct / total\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%, Testing Time: {testing_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrNrGZ04Ax1r"
      },
      "source": [
        "## SqueezeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "appSSIdPAx18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "\n",
        "# Load pre-trained squeezenet model\n",
        "squeezenet = models.squeezenet1_1(weights='SqueezeNet1_1_Weights.DEFAULT')\n",
        "\n",
        "# Freeze feature extraction layers (optional)\n",
        "squeezenet.classifier._modules[\"1\"] = torch.nn.Conv2d(512, 5, kernel_size=(1, 1))\n",
        "squeezenet.num_classes = 4\n",
        "for param in squeezenet.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in squeezenet.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Define optimizer (e.g., Adam optimizer)\n",
        "optimizer = torch.optim.Adam(squeezenet.parameters(), lr=0.01)  # Adjust learning rate as needed\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "\n",
        "num_epochs = 50\n",
        "squeezenet.to(device)\n",
        "for epoch in range(40,num_epochs):\n",
        "    # Train and validate the squeezenet\n",
        "    squeezenet.train()\n",
        "    start = time.time()\n",
        "    train_loss = 0.0\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    batch = 0;\n",
        "    for image, label in train_loader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      batch += 1\n",
        "      print(f'Batch {batch}/{num_batches}',end='')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = squeezenet(image)\n",
        "\n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      print(f'\\r',end='')\n",
        "\n",
        "    # Validate the model\n",
        "    squeezenet.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for image, label in val_loader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            output = squeezenet(image)\n",
        "            loss = criterion(output, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},\\n\\t\\t Val Accuracy: {val_accuracy:.4f}, Time Taken: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained squeezenet\n",
        "torch.save(squeezenet.state_dict(), \"/content/CardioCare/squeezenet.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5WrlICQAx1-"
      },
      "outputs": [],
      "source": [
        "# Testing the model\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = squeezenet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100 * correct / total\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%, Testing Time: {testing_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InceptionV3\n"
      ],
      "metadata": {
        "id": "E3P1E9qQR-Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "Z56phCOKe-bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import inception_v3\n",
        "\n",
        "# Assuming you have defined your train_loader and val_loader\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device used: {device}\")\n",
        "\n",
        "# Load pre-trained InceptionV3 model\n",
        "inceptionv3 = inception_v3(weights='DEFAULT')\n",
        "\n",
        "# Modify the last fully connected layer to match the number of classes in your dataset\n",
        "num_classes = 4  # Adjust according to your dataset\n",
        "num_ftrs = inceptionv3.fc.in_features\n",
        "inceptionv3.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer (e.g., Adam optimizer)\n",
        "optimizer = optim.Adam(inceptionv3.parameters())\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "num_epochs = 30\n",
        "\n",
        "inceptionv3.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Train and validate the InceptionV3 model\n",
        "    inceptionv3.train()\n",
        "    start = time.time()\n",
        "    train_loss = 0.0\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    batch = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        batch += 1\n",
        "        print(f'Batch {batch}/{num_batches}', end='')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = inceptionv3(image)\n",
        "\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        print(f'\\r', end='')\n",
        "\n",
        "    # Validate the model\n",
        "    inceptionv3.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in val_loader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            output = inceptionv3(image)\n",
        "            loss = criterion(output, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},\\n\\t\\t Val Accuracy: {val_accuracy:.4f}, Time Taken: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained InceptionV3 model\n",
        "torch.save(inceptionv3.state_dict(), \"/content/CardioCare/inceptionv3.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jbRguspESAvT",
        "outputId": "f399daa8-c452-43e6-d85c-240f943a8c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cuda\n",
            "Epoch: 1\n",
            "Epoch: 1/30, Train Loss: 22.5622, Val Loss: 1772.1541,\n",
            "\t\t Val Accuracy: 37.0370, Time Taken: 72.85 seconds\n",
            "Epoch: 2\n",
            "Epoch: 2/30, Train Loss: 15.6852, Val Loss: 105.3001,\n",
            "\t\t Val Accuracy: 25.1543, Time Taken: 71.75 seconds\n",
            "Epoch: 3\n",
            "Epoch: 3/30, Train Loss: 15.7522, Val Loss: 29.5353,\n",
            "\t\t Val Accuracy: 38.2716, Time Taken: 68.53 seconds\n",
            "Epoch: 4\n",
            "Epoch: 4/30, Train Loss: 15.1942, Val Loss: 28.8639,\n",
            "\t\t Val Accuracy: 31.3272, Time Taken: 71.07 seconds\n",
            "Epoch: 5\n",
            "Epoch: 5/30, Train Loss: 15.8285, Val Loss: 29.0630,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 72.04 seconds\n",
            "Epoch: 6\n",
            "Epoch: 6/30, Train Loss: 15.3377, Val Loss: 40.8410,\n",
            "\t\t Val Accuracy: 16.9753, Time Taken: 67.67 seconds\n",
            "Epoch: 7\n",
            "Epoch: 7/30, Train Loss: 15.3710, Val Loss: 28.6888,\n",
            "\t\t Val Accuracy: 29.1667, Time Taken: 72.65 seconds\n",
            "Epoch: 8\n",
            "Epoch: 8/30, Train Loss: 15.3406, Val Loss: 28.2399,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 68.91 seconds\n",
            "Epoch: 9\n",
            "Epoch: 9/30, Train Loss: 15.1128, Val Loss: 28.4384,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 71.06 seconds\n",
            "Epoch: 10\n",
            "Epoch: 10/30, Train Loss: 15.0393, Val Loss: 28.4281,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 73.87 seconds\n",
            "Epoch: 11\n",
            "Epoch: 11/30, Train Loss: 15.0529, Val Loss: 29.3443,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 70.01 seconds\n",
            "Epoch: 12\n",
            "Epoch: 12/30, Train Loss: 14.8392, Val Loss: 28.5119,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 73.03 seconds\n",
            "Epoch: 13\n",
            "Epoch: 13/30, Train Loss: 15.0151, Val Loss: 34.9497,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 73.86 seconds\n",
            "Epoch: 14\n",
            "Epoch: 14/30, Train Loss: 14.9275, Val Loss: 28.9107,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 77.95 seconds\n",
            "Epoch: 15\n",
            "Epoch: 15/30, Train Loss: 15.3144, Val Loss: 29.9788,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 68.03 seconds\n",
            "Epoch: 16\n",
            "Epoch: 16/30, Train Loss: 14.9544, Val Loss: 28.7506,\n",
            "\t\t Val Accuracy: 30.5556, Time Taken: 66.49 seconds\n",
            "Epoch: 17\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8c6e9c0e392f>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing\n"
      ],
      "metadata": {
        "id": "1HwbkobLe4Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = inceptionv3(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100 * correct / total\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%, Testing Time: {testing_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTu2SSLWb6Dq",
        "outputId": "aa0ed188-1b35-4d41-c3cf-84dc0e5b00b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Test Loss: 1.3756, Test Accuracy: 30.56%, Testing Time: 44.96 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3As8VCEH4Xm8"
      },
      "source": [
        "## CustomCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmcbhhGh4fOw"
      },
      "source": [
        "### Defining CustomCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iLK2ks24d5M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomCNN, self).__init__()\n",
        "\n",
        "    #define the  layers\n",
        "    self.conv01 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
        "    self.maxpool01 = nn.MaxPool2d(kernel_size=6,stride=3)\n",
        "    self.conv02 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1)\n",
        "    self.maxpool02 = nn.MaxPool2d(kernel_size=6, stride=3)\n",
        "    self.conv03 = nn.Conv2d(in_channels = 128, out_channels=224, kernel_size=3, stride=2, padding=1)\n",
        "    self.maxpool03 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
        "    self.fc01 = nn.Linear(224*224*3,16)\n",
        "    self.dropout01_fc = nn.Dropout2d(0.2)\n",
        "    self.conv04 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=2, padding=2, stride=2)\n",
        "    self.conv05 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=3, padding=2, stride=2)\n",
        "    self.dropout02 = nn.Dropout(0.2)\n",
        "    self.dropout03 = nn.Dropout(0.2)\n",
        "    self.conv07 = nn.Conv2d(in_channels=320, out_channels=256, kernel_size=1, stride=1)\n",
        "    self.dropout07 = nn.Dropout(0.3)\n",
        "    self.fc02 = nn.Linear(2*2*256,512)\n",
        "    self.dropout02_fc = nn.Dropout(0.3)\n",
        "    self.fc03 = nn.Linear(512,4)\n",
        "\n",
        "    self.batchnorm01_fc = nn.BatchNorm2d(16)\n",
        "    self.batchnorm05 = nn.BatchNorm2d(64)\n",
        "    self.batchnorm04 = nn.BatchNorm2d(32)\n",
        "    self.batchnorm01 = nn.BatchNorm2d(64)\n",
        "    self.batchnorm02 = nn.BatchNorm2d(128)\n",
        "    self.batchnorm03 = nn.BatchNorm2d(224)\n",
        "    self.batchnorm07 = nn.BatchNorm2d(256)\n",
        "    self.batchnorm02_fc = nn.BatchNorm2d(512)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #input is branched into 2, branch1 (b1) and branch2 (b2)\n",
        "\n",
        "    #Branch 1\n",
        "    b1 = self.fc01(x.view(x.size(0), -1))\n",
        "    b1 = F.leaky_relu(b1,0.1)\n",
        "\n",
        "    b1 = b1.unsqueeze(2).unsqueeze(3)\n",
        "    # Apply batch normalization\n",
        "    b1 = self.batchnorm01_fc(b1)\n",
        "    # Remove the dummy dimensions after batch normalization\n",
        "    #b1 = b1.squeeze(2).squeeze(2)\n",
        "    b1 = self.dropout01_fc(b1)\n",
        "    #Branch 1a\n",
        "    b1a = self.conv05(b1)\n",
        "    b1a = F.leaky_relu(b1a,0.1)\n",
        "    b1a = self.batchnorm05(b1a)\n",
        "    #Branch 1b\n",
        "    b1b = self.conv04(b1)\n",
        "    b1b = F.leaky_relu(b1b,0.1)\n",
        "    b1b = self.batchnorm04(b1b)\n",
        "    #Merge Branch1a and Branch1b\n",
        "    b1 = torch.cat((b1a,b1b),dim=1)\n",
        "    b1 = self.dropout02(b1)\n",
        "\n",
        "\n",
        "    #Branch 2\n",
        "    b2 = self.conv01(x)\n",
        "    b2 = F.leaky_relu(b2,0.1)\n",
        "    b2 = self.batchnorm01(b2)\n",
        "    b2 = self.maxpool01(b2)\n",
        "    b2 = self.conv02(b2)\n",
        "    b2 = F.leaky_relu(b2,0.1)\n",
        "    b2 = self.batchnorm02(b2)\n",
        "    b2 = self.maxpool02(b2)\n",
        "    b2 = self.conv03(b2)\n",
        "    b2 = F.leaky_relu(b2,0.1)\n",
        "    b2 = self.batchnorm03(b2)\n",
        "    b2 = self.maxpool03(b2)\n",
        "\n",
        "\n",
        "    #Merge Branch 1 and Branch 2\n",
        "    x = torch.cat((b1,b2),dim=1)\n",
        "    x = self.dropout03(x)\n",
        "    x = self.conv07(x)\n",
        "    x = F.leaky_relu(x,0.1)\n",
        "    x = self.batchnorm07(x)\n",
        "    x = self.dropout07(x)\n",
        "\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc02(x)\n",
        "    x = F.leaky_relu(x,0.1)\n",
        "\n",
        "    x = x.unsqueeze(2).unsqueeze(3)\n",
        "    # Apply batch normalization\n",
        "    x = self.batchnorm02_fc(x)\n",
        "    # Remove the dummy dimensions after batch normalization\n",
        "    x = x.squeeze(2).squeeze(2)\n",
        "\n",
        "    x = self.dropout02(x)\n",
        "    x = F.softmax(x,dim=-1)\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "VNxdCbYDnosZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "cardiocare = CustomCNN()\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "# Define optimizer (e.g., Adam optimizer)\n",
        "optimizer = torch.optim.Adam(cardiocare.parameters(), lr=0.1)  # Adjust learning rate as needed\n",
        "\n",
        "num_batches = len(train_loader)\n",
        "\n",
        "num_epochs = 30\n",
        "cardiocare.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    # Train and validate the cardiocare\n",
        "    cardiocare.train()\n",
        "    start = time.time()\n",
        "    train_loss = 0.0\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    batch = 0;\n",
        "    for image, label in train_loader:\n",
        "      image, label = image.to(device), label.to(device)\n",
        "      batch += 1\n",
        "      print(f'Batch {batch}/{num_batches}',end='')\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = cardiocare(image)\n",
        "\n",
        "      loss = criterion(output, label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      print(f'\\r',end='')\n",
        "\n",
        "    # Validate the model\n",
        "    cardiocare.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for image, label in val_loader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            output = cardiocare(image)\n",
        "            loss = criterion(output, label)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += label.size(0)\n",
        "            correct += (predicted == label).sum().item()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f},\\n\\t\\t Val Accuracy: {val_accuracy:.4f}, Time Taken: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# Save the trained cardiocare\n",
        "torch.save(cardiocare.state_dict(), \"/content/CardioCare/cardiocare.pth\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "PIz4BpsSnJT6",
        "outputId": "7a9e312b-29ec-4e66-f703-ff7edd1c2db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide used: cuda\n",
            "Epoch: 1\n",
            "Epoch: 1/30, Train Loss: 18.7143, Val Loss: 37.4500,\n",
            "\t\t Val Accuracy: 0.0000, Time Taken: 57.79 seconds\n",
            "Epoch: 2\n",
            "Epoch: 2/30, Train Loss: 18.6795, Val Loss: 37.4497,\n",
            "\t\t Val Accuracy: 0.0000, Time Taken: 54.28 seconds\n",
            "Epoch: 3\n",
            "Epoch: 3/30, Train Loss: 18.6343, Val Loss: 37.4500,\n",
            "\t\t Val Accuracy: 0.0000, Time Taken: 56.28 seconds\n",
            "Epoch: 4\n",
            "Epoch: 4/30, Train Loss: 18.5932, Val Loss: 37.4500,\n",
            "\t\t Val Accuracy: 0.0000, Time Taken: 55.52 seconds\n",
            "Epoch: 5\n",
            "Epoch: 5/30, Train Loss: 18.5492, Val Loss: 37.4497,\n",
            "\t\t Val Accuracy: 0.0000, Time Taken: 55.07 seconds\n",
            "Epoch: 6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4a9d999c5af4>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcardiocare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing CustomCNN"
      ],
      "metadata": {
        "id": "Pzm-oh9zoFAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cardiocare.state_dict(), \"/content/CardioCare/cardiocare.pth\")\n",
        "print(\"Model saved successfully!\")\n",
        "# Testing the model\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = cardiocare(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = 100 * correct / total\n",
        "\n",
        "end_time = time.time()\n",
        "testing_time = end_time - start_time\n",
        "\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%, Testing Time: {testing_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Ts2P3joEFF",
        "outputId": "5f413d2b-b61c-49cb-d1ea-cddeb610444e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n",
            "Test Loss: 6.2409, Test Accuracy: 0.00%, Testing Time: 29.26 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "DHW8ayO-s416"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing AlexNet"
      ],
      "metadata": {
        "id": "tMs0629JtGQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device used: {device}\")\n",
        "# Load pre-trained AlexNet model\n",
        "alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "# Freeze feature extraction layers (optional)\n",
        "for param in alexnet.features[:].parameters():\n",
        "    param.requires_grad = False  # Freeze weights of feature extraction layers\n",
        "# Get the number of features output by the pre-trained alexnet\n",
        "num_ftrs = alexnet.classifier[6].in_features\n",
        "num_classes = 4\n",
        "alexnet.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "alexnet.load_state_dict(torch.load('/content/CardioCare/alexnet.pth',map_location=device))\n",
        "\n",
        "# Initialize variables for precision, recall, F1-score, and accuracy\n",
        "precision = 0.0\n",
        "recall = 0.0\n",
        "f1_score = 0.0\n",
        "accuracy = 0.0\n",
        "\n",
        "# Initialize variables for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test dataset to compute TP, FP, FN\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = alexnet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "        fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "        fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "if tp + fp > 0:\n",
        "    precision = tp / (tp + fp)\n",
        "if tp + fn > 0:\n",
        "    recall = tp / (tp + fn)\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print precision, recall, F1-score, and accuracy\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb9eE0Vds78o",
        "outputId": "6c300081-9104-4e6d-b59a-f92d1ed61042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide used: cpu\n",
            "Precision: 0.9940\n",
            "Recall: 1.0000\n",
            "F1-score: 0.9970\n",
            "Accuracy: 98.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing SqueezeNet"
      ],
      "metadata": {
        "id": "snvjIQtEyc9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "# Load pre-trained squeezenet model\n",
        "squeezenet = models.squeezenet1_1(weights='SqueezeNet1_1_Weights.DEFAULT')\n",
        "\n",
        "# Freeze feature extraction layers (optional)\n",
        "squeezenet.classifier._modules[\"1\"] = torch.nn.Conv2d(512, 5, kernel_size=(1, 1))\n",
        "squeezenet.num_classes = 4\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "squeezenet.load_state_dict(torch.load('/content/CardioCare/squeezenet.pth',map_location=device))\n",
        "\n",
        "# Initialize variables for precision, recall, F1-score, and accuracy\n",
        "precision = 0.0\n",
        "recall = 0.0\n",
        "f1_score = 0.0\n",
        "accuracy = 0.0\n",
        "\n",
        "# Initialize variables for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test dataset to compute TP, FP, FN\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = squeezenet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "        fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "        fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "if tp + fp > 0:\n",
        "    precision = tp / (tp + fp)\n",
        "if tp + fn > 0:\n",
        "    recall = tp / (tp + fn)\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print precision, recall, F1-score, and accuracy\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b713234-3b51-4233-82c6-de0de2400cbc",
        "id": "NeIx8vFjyc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide used: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n",
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 32.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9371\n",
            "Recall: 0.9504\n",
            "F1-score: 0.9437\n",
            "Accuracy: 79.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing CustomCNN\n",
        "\n"
      ],
      "metadata": {
        "id": "uUHOkPhfzgtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "# Load pre-trained cardiocare model\n",
        "cardiocare = CustomCNN()\n",
        "\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "cardiocare.load_state_dict(torch.load('/content/CardioCare/cardiocare.pth',map_location=device))\n",
        "\n",
        "# Initialize variables for precision, recall, F1-score, and accuracy\n",
        "precision = 0.0\n",
        "recall = 0.0\n",
        "f1_score = 0.0\n",
        "accuracy = 0.0\n",
        "\n",
        "# Initialize variables for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test dataset to compute TP, FP, FN\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = cardiocare(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "        fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "        fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "if tp + fp > 0:\n",
        "    precision = tp / (tp + fp)\n",
        "if tp + fn > 0:\n",
        "    recall = tp / (tp + fn)\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print precision, recall, F1-score, and accuracy\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "b24c2c77-a4d4-4c27-9b7b-4efb5234c8ee",
        "id": "esFwpzGyzgtm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide used: cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/CardioCare/cardiocare.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3c6c41fa053c>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Define loss function (e.g., Cross-Entropy loss for classification)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcardiocare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/CardioCare/cardiocare.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize variables for precision, recall, F1-score, and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/CardioCare/cardiocare.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing InceptionV3\n",
        "\n"
      ],
      "metadata": {
        "id": "oizNVYja0Mbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import inception_v3\n",
        "import math\n",
        "import time\n",
        "\n",
        "# Define device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Devide used: {device}\")\n",
        "# Load pre-trained cardiocare model\n",
        "inceptionv3 = inception_v3(weights='DEFAULT')\n",
        "\n",
        "# Modify the last fully connected layer to match the number of classes in your dataset\n",
        "num_classes = 4  # Adjust according to your dataset\n",
        "num_ftrs = inceptionv3.fc.in_features\n",
        "inceptionv3.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "# Define loss function (e.g., Cross-Entropy loss for classification)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "inceptionv3.load_state_dict(torch.load('/content/CardioCare/inceptionv3.pth',map_location=device))\n",
        "\n",
        "# Initialize variables for precision, recall, F1-score, and accuracy\n",
        "precision = 0.0\n",
        "recall = 0.0\n",
        "f1_score = 0.0\n",
        "accuracy = 0.0\n",
        "\n",
        "# Initialize variables for true positives, false positives, and false negatives\n",
        "tp = 0\n",
        "fp = 0\n",
        "fn = 0\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test dataset to compute TP, FP, FN\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs,_ = inceptionv3(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        tp += ((predicted == 1) & (labels == 1)).sum().item()\n",
        "        fp += ((predicted == 1) & (labels == 0)).sum().item()\n",
        "        fn += ((predicted == 0) & (labels == 1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "if tp + fp > 0:\n",
        "    precision = tp / (tp + fp)\n",
        "if tp + fn > 0:\n",
        "    recall = tp / (tp + fn)\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print precision, recall, F1-score, and accuracy\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1_score:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3e7b26-a28c-4c5a-869b-0dc17b951eeb",
        "id": "RtxdRrCy0McA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Devide used: cpu\n",
            "Precision: 0.0000\n",
            "Recall: 0.0000\n",
            "F1-score: 0.0000\n",
            "Accuracy: 29.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Models"
      ],
      "metadata": {
        "id": "FI6DVnpt6Mt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extractor"
      ],
      "metadata": {
        "id": "EMIVgHre6iIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "def extract_features(dataloader):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Device used: {device}\")\n",
        "  alexnet = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "  num_ftrs = alexnet.classifier[6].in_features\n",
        "  num_classes = 4\n",
        "  alexnet.classifier[6] = torch.nn.Linear(num_ftrs, num_classes)\n",
        "  features = []\n",
        "  labels = []\n",
        "  alexnet.eval()\n",
        "  alexnet = nn.Sequential(*list(alexnet.features.children()))\n",
        "  with torch.no_grad():\n",
        "      for images, targets in dataloader:\n",
        "          features_batch = alexnet(images).detach().numpy()\n",
        "          features.append(features_batch)\n",
        "          labels.append(targets.numpy())\n",
        "  features = np.concatenate(features)\n",
        "  labels = np.concatenate(labels)\n",
        "\n",
        "  features = features.reshape(features.shape[0], -1)\n",
        "  return features, labels\n",
        "\n",
        "# Extract features from the dataset\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)"
      ],
      "metadata": {
        "id": "ENoO7TFd6l9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "4uzMAE9478vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXHLArZs78Ln",
        "outputId": "79254045-02ab-40aa-8c1a-de2c3e7e841b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cpu\n",
            "Device used: cpu\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "PrKrS33y_gNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2-MAGVC_irw",
        "outputId": "fc321c8f-3e5d-44c0-94db-126a5717435d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree"
      ],
      "metadata": {
        "id": "FZJpBSDu_vO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWBT3gQJ___D",
        "outputId": "baeecd31-4502-4378-e362-8bf4c18889be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "lWHq5EogAOx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Extract features from the dataset\n",
        "features, labels = extract_features(train_loader)\n",
        "\n",
        "# Reshape the features\n",
        "num_samples = features.shape[0]\n",
        "features = features.reshape(num_samples, -1)  # Flatten features\n",
        "\n",
        "# Step 2: Prepare data for kNN\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train kNN Classifier\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors as needed\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the trained model\n",
        "y_pred = knn_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A467XNz2AN1y",
        "outputId": "7a0c6243-8561-4565-8108-ddd10fd6dc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cpu\n",
            "Accuracy: 0.823076923076923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save ML Models"
      ],
      "metadata": {
        "id": "Wuei-hFaAgIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save Random Forest Classifier\n",
        "joblib.dump(rf_classifier, '/content/CardioCare/random_forest_model.pkl')\n",
        "\n",
        "# Save Decision Tree Classifier\n",
        "joblib.dump(dt_classifier, '/content/CardioCare/decision_tree_model.pkl')\n",
        "\n",
        "# Save kNN Classifier\n",
        "joblib.dump(knn_classifier, '/content/CardioCare/knn_model.pkl')\n",
        "\n",
        "# Save SVM Classifier\n",
        "joblib.dump(svm_classifier, '/content/CardioCare/svm_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGLwQPVfAh91",
        "outputId": "2305f3ca-6b34-4686-f20e-8be6f159145a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/CardioCare/svm_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "hhkX4P4XBHWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Load the saved models\n",
        "loaded_rf_model = joblib.load('random_forest_model.pkl')\n",
        "loaded_dt_model = joblib.load('decision_tree_model.pkl')\n",
        "loaded_knn_model = joblib.load('knn_model.pkl')\n",
        "loaded_svm_model = joblib.load('svm_model.pkl')\n",
        "\n",
        "# Define a function to evaluate a model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "# Evaluate all four models\n",
        "rf_precision, rf_recall, rf_f1, rf_accuracy = evaluate_model(loaded_rf_model, X_test, y_test)\n",
        "dt_precision, dt_recall, dt_f1, dt_accuracy = evaluate_model(loaded_dt_model, X_test, y_test)\n",
        "knn_precision, knn_recall, knn_f1, knn_accuracy = evaluate_model(loaded_knn_model, X_test, y_test)\n",
        "svm_precision, svm_recall, svm_f1, svm_accuracy = evaluate_model(loaded_svm_model, X_test, y_test)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(\"Random Forest:\")\n",
        "print(\"Precision:\", rf_precision)\n",
        "print(\"Recall:\", rf_recall)\n",
        "print(\"F1-score:\", rf_f1)\n",
        "print(\"Accuracy:\", rf_accuracy)\n",
        "print()\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(\"Precision:\", dt_precision)\n",
        "print(\"Recall:\", dt_recall)\n",
        "print(\"F1-score:\", dt_f1)\n",
        "print(\"Accuracy:\", dt_accuracy)\n",
        "print()\n",
        "\n",
        "print(\"k-Nearest Neighbors:\")\n",
        "print(\"Precision:\", knn_precision)\n",
        "print(\"Recall:\", knn_recall)\n",
        "print(\"F1-score:\", knn_f1)\n",
        "print(\"Accuracy:\", knn_accuracy)\n",
        "print()\n",
        "\n",
        "print(\"Support Vector Machine:\")\n",
        "print(\"Precision:\", svm_precision)\n",
        "print(\"Recall:\", svm_recall)\n",
        "print(\"F1-score:\", svm_f1)\n",
        "print(\"Accuracy:\", svm_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiIoZptaBKqR",
        "outputId": "fed646ad-9e41-4ae6-d3a5-db452695077d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest:\n",
            "Precision: 0.9264145610487072\n",
            "Recall: 0.9230769230769231\n",
            "F1-score: 0.922073863334659\n",
            "Accuracy: 0.9230769230769231\n",
            "\n",
            "Decision Tree:\n",
            "Precision: 0.9027725479648556\n",
            "Recall: 0.9\n",
            "F1-score: 0.8986886025800054\n",
            "Accuracy: 0.9\n",
            "\n",
            "k-Nearest Neighbors:\n",
            "Precision: 0.8370738228064926\n",
            "Recall: 0.823076923076923\n",
            "F1-score: 0.8183064865991695\n",
            "Accuracy: 0.823076923076923\n",
            "\n",
            "Support Vector Machine:\n",
            "Precision: 0.9848290598290598\n",
            "Recall: 0.9846153846153847\n",
            "F1-score: 0.9846182737450343\n",
            "Accuracy: 0.9846153846153847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Training"
      ],
      "metadata": {
        "id": "2U6kP-_cfX5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defense Training"
      ],
      "metadata": {
        "id": "Pgq5JkVWfdN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torchattacks\n",
        "import math\n",
        "\n",
        "model = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "num_ftrs = model.classifier[6].in_features\n",
        "num_classes = 4\n",
        "model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "\n",
        "last_fc_idx = len(list(model.features.children()))\n",
        "print(\"Total number of layers in features: \", last_fc_idx)\n",
        "\n",
        "# Freeze most layers except the last 30% of features\n",
        "for idx, layer in enumerate(model.features.children()):\n",
        "    if idx < (last_fc_idx * 0.7):\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_samples = len(train_dataset)\n",
        "batch_size = train_loader.batch_size\n",
        "num_batches = math.ceil(num_samples / batch_size)\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load('/content/CardioCare/alexnet.pth'))\n",
        "model = model.to(device)\n",
        "print(\"Device used: \",device)\n",
        "# Step 4: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "num_epochs = 15\n",
        "model.train()\n",
        "# Define the PGD attack\n",
        "pgd = torchattacks.PGD(model, eps=0.007, alpha=0.002,steps=8)\n",
        "pgd.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #setting normalization used to the attack object\n",
        "fgsm = torchattacks.FGSM(model, eps=0.02)\n",
        "fgsm.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #setting normalization used to the attack object\n",
        "df = torchattacks.DeepFool(model,steps=10)\n",
        "df.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #setting normalization used to the attack object\n",
        "\n",
        "\n",
        "fgsm_fract = 0.5\n",
        "df_fract = 0.5\n",
        "pgd_fract = 0.5\n",
        "\n",
        "f1,f2,f3 = 0,int(batch_size*fgsm_fract),batch_size\n",
        "df1,df2,df3 = 0,int(batch_size*df_fract),batch_size\n",
        "pgd1,pgd2,pgd3 = 0,int(batch_size*pgd_fract),batch_size\n",
        "\n",
        "for epoch in range(num_epochs, num_epochs*2):\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    model.train()\n",
        "    print(f'FGSM Epoch started: {epoch+1}/{num_epochs}')\n",
        "    start_time = time.time()\n",
        "    count = 1\n",
        "    #training against fgsm\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        print(f\"FGSM Batch: {count}/{num_batches}\",end='')\n",
        "        count+=1\n",
        "        # Generate adversarial examples FGSM\n",
        "        fgsm_imgs = fgsm(images[f1:f2], labels[f1:f2]) #next 20 images\n",
        "        fgsm_imgs.to(device)\n",
        "        # Concatenate adversarial examples with clean inputs\n",
        "        inputs = torch.cat([fgsm_imgs, images[f2:f3]], dim=0)\n",
        "\n",
        "        # Perform one step of training\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        print('\\r', end='')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    end_time=time.time()\n",
        "    epoch_run_time=end_time-start_time\n",
        "    accuracy = (correct_predictions / total_predictions) * 100.0\n",
        "    print(f'\\nFGSM Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Time: {time.time() - start_time:.2f} seconds')\n",
        "\n",
        "\n",
        "    print(f'PGD Epoch started: {epoch+1}/{num_epochs}')\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    count=1\n",
        "    start_time = time.time()\n",
        "    #training against PGD\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        print(f\"Batch: {count}/{num_batches}\",end='')\n",
        "        count+=1\n",
        "        # Generate adversarial examples FGSM\n",
        "        pgd_imgs = pgd(images[pgd1:pgd2], labels[pgd1:pgd2]) #next 20 images\n",
        "        pgd_imgs.to(device)\n",
        "        # Concatenate adversarial examples with clean inputs\n",
        "        inputs = torch.cat([pgd_imgs, images[pgd2:pgd3]], dim=0)\n",
        "\n",
        "        # Perform one step of training\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        print('\\r', end='')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    end_time=time.time()\n",
        "    epoch_run_time=end_time-start_time\n",
        "    accuracy = (correct_predictions / total_predictions) * 100.0\n",
        "    print(f'\\nPGD Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Time: {time.time() - start_time:.2f} seconds')\\\n",
        "\n",
        "    print(f'DeepFool Epoch started: {epoch+1}/{num_epochs}')\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    count=1\n",
        "    start_time = time.time()\n",
        "    #training against deepfool\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        print(f\"Batch: {count}/{num_batches}\",end='')\n",
        "        count+=1\n",
        "        # Generate adversarial examples FGSM\n",
        "        df_imgs = df(images[df1:df2], labels[df1:df2]) #next 20 images\n",
        "        df_imgs.to(device)\n",
        "        # Concatenate adversarial examples with clean inputs\n",
        "        inputs = torch.cat([df_imgs, images[df2:df3]], dim=0)\n",
        "\n",
        "        # Perform one step of training\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        logits = outputs\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "        print('\\r', end='')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    end_time=time.time()\n",
        "    epoch_run_time=end_time-start_time\n",
        "    accuracy = (correct_predictions / total_predictions) * 100.0\n",
        "    print(f'\\nDeepFool Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Time: {time.time() - start_time:.2f} seconds')\n",
        "\n",
        "torch.save(model.state_dict(), '/content/CardioCare/advtrained.pth')\n"
      ],
      "metadata": {
        "id": "8aHXqJ1fha2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defense Testing"
      ],
      "metadata": {
        "id": "sSuL23Umx-oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import torchattacks\n",
        "import os\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_accuracy_2(model):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # Step 4:attack objects\n",
        "  fgsm = torchattacks.FGSM(model,eps=0.02)\n",
        "  fgsm.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  df = torchattacks.DeepFool(model)\n",
        "  df.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  pgd = torchattacks.PGD(model,eps=0.007,alpha=0.002)\n",
        "  pgd.set_normalization_used(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "  # Replace the generator expressions with explicit tensors for mean and std\n",
        "  mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
        "  std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
        "\n",
        "\n",
        "  correct_original = 0\n",
        "  correct_adversarial_fgsm = 0\n",
        "  correct_adversarial_pgd = 0\n",
        "  correct_adversarial_df = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  pure_loss = 0.0\n",
        "  fgsm_loss = 0.0\n",
        "  pgd_loss = 0.0\n",
        "  df_loss = 0.0\n",
        "\n",
        "  num_samples = len(test_dataset)\n",
        "  batch_size = test_loader.batch_size\n",
        "  num_batches = math.ceil(num_samples / batch_size)\n",
        "\n",
        "  half_batch, batch = batch_size//2, batch_size\n",
        "\n",
        "  count = 1\n",
        "  for images, labels in test_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      print(f'Batch: {count}/{num_batches}',end='')\n",
        "      count+=1\n",
        "      # Original classification\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      pure_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      correct_original += (predicted == labels).sum().item()\n",
        "\n",
        "      # Adversarial attack FGSM\n",
        "      adversarial_images = fgsm(images[:half_batch], labels[:half_batch])\n",
        "      adversarial_images = torch.cat((adversarial_images,images[half_batch:batch]),dim=0)\n",
        "      outputs_adv = model(adversarial_images)\n",
        "      loss = criterion(outputs_adv, labels)\n",
        "      fgsm_loss += loss.item()\n",
        "      _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
        "      correct_adversarial_fgsm += (predicted_adv == labels).sum().item()\n",
        "\n",
        "      # Adversarial attack PGD\n",
        "      adversarial_images = pgd(images[:half_batch], labels[:half_batch])\n",
        "      adversarial_images = torch.cat((adversarial_images,images[half_batch:batch]),dim=0)\n",
        "      outputs_adv = model(adversarial_images)\n",
        "      loss = criterion(outputs_adv, labels)\n",
        "      pgd_loss += loss.item()\n",
        "      _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
        "      correct_adversarial_pgd += (predicted_adv == labels).sum().item()\n",
        "\n",
        "      # Adversarial attack DeepFool\n",
        "      adversarial_images = df(images[:half_batch], labels[:half_batch])\n",
        "      adversarial_images = torch.cat((adversarial_images,images[half_batch:batch]),dim=0)\n",
        "      outputs_adv = model(adversarial_images)\n",
        "      loss = criterion(outputs_adv, labels)\n",
        "      df_loss += loss.item()\n",
        "      _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
        "      correct_adversarial_df += (predicted_adv == labels).sum().item()\n",
        "\n",
        "      total_samples += labels.size(0)\n",
        "      print('\\r',end='')\n",
        "\n",
        "  accuracy_original = 100 * correct_original / total_samples\n",
        "  accuracy_adversarial_fgsm = 100 * correct_adversarial_fgsm / total_samples\n",
        "  accuracy_adversarial_pgd = 100 * correct_adversarial_pgd / total_samples\n",
        "  accuracy_adversarial_df = 100 * correct_adversarial_df / total_samples\n",
        "\n",
        "  pure_loss /= len(test_loader)\n",
        "  fgsm_loss /= len(test_loader)\n",
        "  pgd_loss /= len(test_loader)\n",
        "  df_loss /= len(test_loader)\n",
        "\n",
        "  print(f\"Original images:\\nTest Accuracy = {accuracy_original:.2f}%, Test Loss = {pure_loss:.3f}\")\n",
        "  print(f\"FGSM images:\\nTest Accuracy = {accuracy_adversarial_fgsm:.2f}%, Test Loss = {fgsm_loss:.3f}\")\n",
        "  print(f\"DeepFool images:\\nTest Accuracy = {accuracy_adversarial_df:.2f}%, Test Loss = {df_loss:.3f}\")\n",
        "  print(f\"PGD images:\\nTest Accuracy = {accuracy_adversarial_pgd:.2f}%, Test Loss = {pgd_loss:.3f}\")\n",
        "\n",
        "\n",
        "# Step 3: Model Initialization\n",
        "model = models.alexnet(weights='AlexNet_Weights.DEFAULT')\n",
        "num_ftrs = model.classifier[6].in_features\n",
        "num_classes = 4\n",
        "model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "print(\"Device used: \",device)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/CardioCare/alexnet.pth'))\n",
        "print(\"Baseline Model Accuracy:\")\n",
        "find_accuracy_2(model)\n",
        "\n",
        "print(\"\\n\")\n",
        "model.load_state_dict(torch.load('/content/CardioCare/advtrained.pth'))\n",
        "print(\"Adversarial Trained Accuracy:\")\n",
        "find_accuracy_2(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh-soKM8yA4I",
        "outputId": "74d08b85-db0b-4494-f14a-530a1a2afc57"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used:  cuda\n",
            "Baseline Model Accuracy:\n",
            "Original images:\n",
            "Test Accuracy = 97.69%, Test Loss = 0.080\n",
            "FGSM images:\n",
            "Test Accuracy = 49.07%, Test Loss = 6.544\n",
            "DeepFool images:\n",
            "Test Accuracy = 71.91%, Test Loss = 0.758\n",
            "PGD images:\n",
            "Test Accuracy = 48.46%, Test Loss = 11.798\n",
            "\n",
            "\n",
            "Adversarial Trained Accuracy:\n",
            "Original images:\n",
            "Test Accuracy = 95.06%, Test Loss = 0.397\n",
            "FGSM images:\n",
            "Test Accuracy = 77.01%, Test Loss = 0.740\n",
            "DeepFool images:\n",
            "Test Accuracy = 71.30%, Test Loss = 0.727\n",
            "PGD images:\n",
            "Test Accuracy = 60.65%, Test Loss = 1.348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "ezRpMQoSgHGu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdNO10rH3XYP"
      },
      "source": [
        "## Save Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9QmUgfC3ZRR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive') #mount Google Drive\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def copy_folder_contents(source_folder, destination_folder):\n",
        "    for item in os.listdir(source_folder):\n",
        "        item_path = os.path.join(source_folder, item)\n",
        "        destination_path = os.path.join(destination_folder, item)\n",
        "\n",
        "        if os.path.isfile(item_path):\n",
        "            shutil.copy(item_path, destination_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            os.makedirs(destination_path, exist_ok=True)\n",
        "            copy_folder_contents(item_path, destination_path)\n",
        "\n",
        "\n",
        "\n",
        "source_folder = \"/content/CardioCare\"\n",
        "destination_folder = \"/content/drive/My Drive/CardioCare\"\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "copy_folder_contents(source_folder, destination_folder)\n",
        "\n",
        "\n",
        "print(\"Files Saved!!!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO8M23LF-KXv"
      },
      "source": [
        "## Citations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb53JdWm8sb4"
      },
      "outputs": [],
      "source": [
        "#Khan, Ali Haider; Hussain, Muzammil  (2021), “ECG Images dataset of Cardiac Patients ”, Mendeley Data, V2, doi: 10.17632/gwbz3fsgp8.2"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "3lSYd2mY-vvI",
        "EKc2jG4Qf9Vi",
        "3q2Hj5StYbBe",
        "121I7sZLSix9",
        "hrNrGZ04Ax1r",
        "1HwbkobLe4Yq",
        "3As8VCEH4Xm8",
        "rmcbhhGh4fOw",
        "VNxdCbYDnosZ",
        "DHW8ayO-s416",
        "tMs0629JtGQk",
        "snvjIQtEyc9O",
        "uUHOkPhfzgtV",
        "oizNVYja0Mbu",
        "ezRpMQoSgHGu"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}